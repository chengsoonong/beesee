\documentclass[11pt, oneside]{report}

\usepackage{geometry}
\geometry{a4paper}
\usepackage{amsmath}
% \setmonofont[Scale=0.85]{Menlo}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{pgfplots}
\hypersetup{linktoc=all}
\usepackage[T1]{fontenc}
\usepackage{newpxmath}
\usepackage{color}
\usepackage{mathspec}
\setmainfont [Ligatures={Common,TeX}]{Palatino Linotype} 
\usepackage{setspace}
\usepackage{wrapfig}

\newcommand{\comment}[1]{\textcolor{red}{#1}}
\newcommand{\pcomment}[1]{\textcolor{green}{#1}}
\DeclareMathOperator*{\argmin}{argmin}

\linespread{1.2}

%\renewcommand{\abstractname}{Abstract}



\title{Automated Bee Pose Estimation}
\author{Jakub Nabaglo, u5558578}
\date{Semester 2, 2015}

\begin{document}
\maketitle

\begin{abstract}
    \pcomment{ABSTRACT}
\end{abstract}

\tableofcontents\newpage

\chapter{Motivation}
    Bees play a crucial role in the environment. Acting as important pollinators for numerous flowering crops, they are relied upon by our food supply and our economy, being responsible for an estimated \$200\,billion per annum in crop production[4]. Further, bees form an integral part of our ecosystems.

    Bee populations worldwide have been decreasing for decades. Research into the causes of this decline, and ways of limiting it, is crucial. It is challenged by the difficulty in tracking bee populations. That difficulty is due to a number of factors, including a lack of trained taxonomists as well as limited resources being devoted to recording bee specimens.

    A software tool capable of automatically classifying bee species from images would hence be valuable, allowing scientists and amateurs to document bee sightings by species and thus increasing the amount of data available for research. An extension of this would be a crowdsourcing application that would enable non-professionals to photograph bees that they encouter, storing the specimen species along with a geotag and a timestamp in a public database.

\chapter{Related works}
    Santana et al. (2014) have performed automated bee classification based on bee wing images labelled with the bee species and annotated with landmarks. Their process extracts wing features such as `vein length, width, curvature, angles and the area of all cells' (Santana et al, 2014, p. 253) that may be input into a generic classifier.

    \begin{wrapfigure}{l}{0.5\textwidth}
        \includegraphics[width=0.5\textwidth]{santana.jpg}
        \caption{Image typical of the dataset used by Santana et al. (2014)}
        \label{fig:santana_example}
    \end{wrapfigure}

    Their approach presents several problems. In order to reliably extract features, the training and testing photographs must be captured in a highly uniform manner: the wing images must be of a high resolution and have a plain white background. An example image is shown in figure \ref{fig:santana_example}. If the goal is to enable amateurs to photograph bees and automatically classify them, the user cannot be expected to possess the equipment or motivation necessary for such consistent specimen collection.

    Further, in the work of Santana et al. (2014), images used for training and prediction must be annotated with landmarks before relevant features can be extracted. This, again, is not something an an amateur user should be expected to do.

    As such, it is necessary to develop a method to classify bees regarding of their position or orientation in the image, without requiring the user to take the photograph in a particular way, and not necessitating that they annotate landmarks. It is a logical first step to train a machine to detect the pose of the bee, as well as the bee parts useful for classification.

    Yang and Ramanan (2011) describe a model for human pose estimation. It is a supervised learning model that trains on photographs of humans that are annotated with positions of their body parts. In addition, a tree model must be defined on these parts. The tree is used in a graphical model to take advantage of relationships between body parts during training and detection.

    Although their work describes human pose estimation, it may conceivably be applied to bee pose estimation. Challenges may include the small size of some bee parts, such as antennae or legs, as well as the tendency for bee images to occlude parts: these may lower the accuracy of part detections. Large variations in bee poses may also be problematic, as this reduces the utility of the graphical model.

    \pcomment{EXPAND}

\chapter{Method}
    Experiments to perform bee pose estimation were conducted on the BEES dataset. From the BEES dataset, 250 images with all seven parts labelled were selected and split into five sets of 50. Of these, four were used for training and one was used for testing. The images in BEES deemed unsuitable for use in experiments due to bee part occlusions were used as a basis for a negative training set: bees were cropped out of these images, leaving only the background.

    The algorithm described by Yang and Ramanan (2011) was used as a basis for the experiments. A tree structure was defined on the seven bee parts labelled in BEES, with the head as the root; the antennae and the thorax as children of the head; and the wings and the abdomen as children of the thorax.

    Following training on the four training sets, predictions were made on the testing set. The testing set predictions were then displayed and visually inspected for correctness and accuracy.

\section{Algorithm}
    The algorithm used is based on the work of Yang and Ramanan (2011). It is described in more detail in their paper. A summary is provided here.

    Let $I$ be an image.

    Let $K \in\mathbb{N}$ be the number of bee parts considered, such as the head, or the wings. Then let $i \in \{1,\dots,K\}$ denote a bee part, such as the head or the left wing.

    Let $T\in\mathbb{N}$ be the number of components in the mixtures model of each part.  Let $t_i \in\{1,\dots,T\}$ be the `type' of part $i$. As an example, a bee wing can be of the folded type or of the extended type. Let $\vec{t}=(t_1,\dots,t_K)^\intercal$.

    Let $p_i=(x_i,y_i)$ be the position of part $i$ in the image $I$. Let $\vec p=(p_1,\dots,p_K)$.

    We wish to assign scores to configurations of parts. For all parts $i, j \in \{1,\dots,K\}$, let $b_i^{t_i}$ be a parameter that is greater for assignments of particular types $t_i$ to $i$. In addition, let $b_{i,j}^{t_i,t_j}$ be greater for particular co-assignments of types $t_i$ and $t_j$ to $i$ and $j$ respectively: for example, one wing being extended may make it more likely that the other wing is also extended. Define the function   \[
        S(\vec t) = \sum_{i\in V}b_i^{t_i}+\sum_{(i,j)\in E}b_{i,j}^{t_i,t_j}\textrm{,}
    \]
    where $(V,E)$ is a $K$-node relational graph, where the nodes correspond to parts and edges correspond to relation constraints between part types.

    It is now possible to define the full score for a configuration of part types $\vec t$, part positions $\vec p$ in an image $I$: \[
        S(I,\vec p, \vec t) = S(\vec t) + \sum_{i\in V}w^{t_i}_i\cdot\phi(I,p_i)
        + \sum_{(i,j) \in E}w_{i,j}^{t_i,t_j}\cdot\psi(p_i-p_j)\textrm{,}
    \]
    where $w_i^{t_i}$ and $w_{i,j}^{t_i,t_k}$ are learned parameters; where $\phi(I, p_i)$ is a HOG descriptor extracted from $p_i$ in $I$; and where $\psi(p_i-p_j)=\left((x_i-x_j)\;(x_i-x_j)^2\;(y_i-y_j)\;(y_i-y_j)^2\right)^\intercal$.

    Observing that $S(I,\vec p, \vec t)$ is linear in parameters $b_i^{t_i}$, $b_{i,j}^{t_i,t_j}$, $w_i^{t_i}$ and $w_{i,h}^{t_i,t_j}$, we can define $\vec\beta = \left(w_i^{t_i}, \dots, w_{i,h}^{t_i,t_j}, \dots, b_i^{t_i}, \dots, b_{i,j}^{t_i,t_j}, \dots\right)^\intercal$ such that $S\left(I, \vec p, \vec t\right)=\vec\beta\cdot\Phi\left(I, \vec p, \vec t\right)$ for some function $\Phi$. Learning then solves the optimisation problem: \begin{align*}
        \argmin_{w_i^{t_i}, \dots, w_{i,h}^{t_i,t_j}, \dots, \xi_i, \ldots \geq 0} &\quad\frac12\beta\cdot\beta+C\sum_n\xi_n\\
        \textrm{s.t.}\quad\forall n \in \textrm{pos} & \quad\beta\cdot\Phi(I_n, p_n, t_n) \geq 1-\xi_n \\
        \forall n \in \textrm{pos} \;\forall (p_i, t_i) & \quad\beta\cdot\Phi(I_n, p_i, t_i) \leq 1-\xi_n\textrm{.}
    \end{align*}
    The above enforces that positive examples should have scores that are greater than 1, while negative examples should score less than $-1$ for all possible part positions and types. $\xi_n$ are slack variables that penalise violations of these constraints. $C>0$ is a constant.

    Inference is then equivalent to maximising $S(I,\vec p, \vec t)$ over $\vec p$ and $\vec t$. This can be done efficiently with dynamic programming.

\section{Models for bees}


\section{Evaluation}
p    \item
        \comment{PCK}
    \item
        \comment{APK}
    \end{itemize}

\chapter{Results}
    \begin{itemize}
    \item
        \comment{box size comparison + graph}
    \item
        \comment{3 vs 7 part model}
    \item
        \comment{w or without augmentation}
    \item
        \comment{comparison to human performance}
    \item
        \comment{average emitting thing}
    \end{itemize}

\chapter{Future direction}
    \begin{itemize}
    \item
        \comment{use in bee classification - reference paper citing important bee parts}
    \item
        \comment{crowdsourcing}
    \end{itemize}

\chapter{References}
\pcomment{use bibtex}

[1] Y. Yang, D. Ramanan. Articulated Pose Estimation using Flexible Mixtures of Parts. CVPR 2011.

[2] http://www.stat.ucla.edu/~xianjie.chen/projects/pose\_estimation/pose\_estimation.html

[3] http://www.pcs.usp.br/~lti/joomla/index.php?option=com\_content\&view=article\&id=23\&Itemid=14

[4] Agriculture and Consumer Protection Department of the Food and Agriculture Organization of the United Nations


\appendix
\chapter{The BEES Dataset}
    The BEES dataset was developed for this project. It is composed of 640 photographs displaying bees of various species in diverse positions.

    Each photograph in the BEES dataset displays one bee. The bee is labelled with the coordinates in the image of the middle of its head, thorax, and abdomen, as well as the tip of its right wing, left wing, right antenna and left antenna. However, in the case that one of these bee parts is occluded, no position is listed for that part. Difficult images are also tagged, and the bee is labelled with its species. Examples shown in figure \ref{fig:BEES_examples}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.424\textwidth]{b1.pdf}\quad\quad\quad
    \includegraphics[width=0.424\textwidth]{b2.pdf}
    \includegraphics[width=0.424\textwidth]{b3.pdf}\quad\quad\quad
    \includegraphics[width=0.424\textwidth]{b4.pdf}
    \caption{Examples of annotated images in the BEES dataset}
    \label{fig:BEES_examples}
\end{figure}

\chapter{Labelling tool}
    \comment{WRITE}

\chapter{Python scripts}
    \section{Visualisation tool}
        \comment{WRITE}

    \section{Shuffling tool}
        \comment{WRITE}
    
    \section{Conversion to .mat tool}
        \comment{WRITE}
    
    \section{Results reader}
        \comment{WRITE}

\end{document}
